{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language modeling\n",
        "## 0: Install dependencies:\n",
        "\n",
        "**You can only use the libraries imported for you in this assignment**"
      ],
      "metadata": {
        "id": "3747Kpgry-91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.24.0 datasets==2.7.0 tqdm==4.64.1 sentencepiece==0.1.97 gensim==4.2.0 apache-beam==2.42.0 sentence-transformers==2.2.2 googledrivedownloader"
      ],
      "metadata": {
        "id": "syRxbsHSy7oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1ab3c2-00d1-425c-86e8-eddd65ba5047"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.24.0 in /usr/local/lib/python3.8/dist-packages (4.24.0)\n",
            "Requirement already satisfied: datasets==2.7.0 in /usr/local/lib/python3.8/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.8/dist-packages (0.1.97)\n",
            "Requirement already satisfied: gensim==4.2.0 in /usr/local/lib/python3.8/dist-packages (4.2.0)\n",
            "Requirement already satisfied: apache-beam==2.42.0 in /usr/local/lib/python3.8/dist-packages (2.42.0)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.8/dist-packages (0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.24.0) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (1.3.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (7.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (0.3.1.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (0.70.9)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (3.8.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.0) (2022.11.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.7.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (1.7)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (1.7.0)\n",
            "Requirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (0.17.4)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (2022.6)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (1.3.0)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (3.8.3)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (1.51.1)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (4.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (3.19.6)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (2.8.2)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (2.7.0)\n",
            "Requirement already satisfied: cloudpickle~=2.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (2.1.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (1.22.1)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.42.0) (0.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers==2.2.2) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers==2.2.2) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers==2.2.2) (1.13.0+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers==2.2.2) (3.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.0) (4.0.2)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.8/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.42.0) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.42.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.24.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.24.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.24.0) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.24.0) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers==2.2.2) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers==2.2.2) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers==2.2.2) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1: Introduction\n",
        "This section give you a quick introduction/refresher to language models"
      ],
      "metadata": {
        "id": "qeMIgxoOy8Un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a language model?\n",
        "A language model is a statistical model that give you the probability of some given text"
      ],
      "metadata": {
        "id": "N1P-nslE4gTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a token?\n",
        "You can't find the probability of most sequences longer than a few words directly since the $26^N$ possible sequences of length N only including the lower case letters in the English alphabet. That number can become astronomically large quickly. \n",
        "\n",
        "Solution: break the text up into small units (tokens)\n",
        "\n",
        "Each token is typically a word or punctuation (but, can be other short sequences of characters)"
      ],
      "metadata": {
        "id": "CmxJcpeD6jyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** a) Finish the implementation exercises\n",
        "#### Your first exercise is to create a tokenizer which take some text as input and outputs a list of tokens\n",
        "\n",
        "To make things a little easier you can assume that all tokens are separated by \" \" or \"-\"\n",
        "\n",
        "You may use the `re` module, but there are simpler solutions that do not need it"
      ],
      "metadata": {
        "id": "0h8MBUfA_j59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A6yRdym0ybj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac162bf1-9eb4-4fc3-cbd3-983dfb666a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Is', 'water', 'a', 'Non', 'Newtonian', 'fluid', '?']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Basic tokenizer function\n",
        "def tokenize(text: str) -> list:\n",
        "  \"\"\"\n",
        "  Input: text, the string to be tokenized\n",
        "  Output: tokens, a list of token strings\n",
        "\n",
        "  Turns text into a list of tokens\n",
        "  \"\"\"\n",
        "  # !!!!!!!!!!!!! Your code starts here !!!!!!!!!!!!! \n",
        "  tokens = re.split(r\"-| \", text)\n",
        "  # !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!! \n",
        "  return tokens\n",
        "\n",
        "example_text = \"Is water a Non-Newtonian fluid ?\"\n",
        "tokenized_example_text = tokenize(example_text)\n",
        "print(tokenized_example_text)\n",
        "# Expected output: ['Is', 'water', 'a', 'Non', 'Newtonian', 'fluid', '?']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the simplest language models is the unigram model. It stores the probability of encountering each token, ignoring surrounding tokens(it does not use conditional probability):\n",
        "\n",
        "$P(sentence)=P(token_1)P(token_2)...P(token_N)$"
      ],
      "metadata": {
        "id": "TDnE_nkZUuQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Unigram:\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializes log probabilities\n",
        "    \"\"\"\n",
        "    self.log_probabilities = {}\n",
        "    self.unknown_log_probability = 0.0\n",
        "\n",
        "  def train(self, sentences: list)->None:\n",
        "    \"\"\"\n",
        "    Input: sentences, list of already tokenized sentences \n",
        "    Ex. [['Hello','my','name','is','HAL'],['Hi','HAL']]\n",
        "\n",
        "    Save log probability of seeing each token using `np.log` to obtain the log probabilities\n",
        "\n",
        "    \"\"\"\n",
        "    # Add a single unknown token\n",
        "    sentences.append(['<unknown token>'])\n",
        "    # !!!!!!!!!!!!! Your code starts here !!!!!!!!!!!!! \n",
        "    total = []\n",
        "    for sentence in sentences:\n",
        "      total = total + sentence\n",
        "    token_num = len(total)\n",
        "    uniqe = set(total)\n",
        "    probabilities = {}\n",
        "    for i in uniqe:\n",
        "      probabilities[i] = 0\n",
        "    for token in total:\n",
        "      probabilities[token] += 1/token_num\n",
        "    for i in uniqe:\n",
        "      self.log_probabilities[i] = np.log(probabilities[i])\n",
        "    # !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!! \n",
        "    # Assign probability for unseen tokens\n",
        "\n",
        "    self.unknown_log_probability = self.log_probabilities.pop('<unknown token>')\n",
        "\n",
        "  def token_log_prob(self, token:str) -> float:\n",
        "    \"\"\"\n",
        "    Get the log probability of a single token with self.unknown_log_probability use if a token was not found during training\n",
        "    \"\"\"\n",
        "    # !!!!!!!!!!!!! Your code starts here !!!!!!!!!!!!! \n",
        "    if token not in list(self.log_probabilities.keys()):\n",
        "      return self.unknown_log_probability\n",
        "    return self.log_probabilities[token]\n",
        "    # !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!! \n",
        "\n",
        "  def sentence_log_prob(self, sentence:list) -> float:\n",
        "    \"\"\"\n",
        "    Get the log probability of an already tokenized sentence\n",
        "    \"\"\"\n",
        "    # !!!!!!!!!!!!! Your code starts here !!!!!!!!!!!!! \n",
        "    s_prob = 0\n",
        "    for token in sentence:\n",
        "      s_prob += self.token_log_prob(token)\n",
        "    return s_prob \n",
        "    # !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!! \n",
        "\n",
        "model = Unigram()\n",
        "model.train([['Hello','my','name','is','HAL'],['Hi','HAL']])\n",
        "print('\"Hello\" log prob:',model.token_log_prob('Hello'))\n",
        "print('\"Hi my name is HAL\" log prob:',model.sentence_log_prob(tokenize(\"Hi my name is HAL\")))"
      ],
      "metadata": {
        "id": "xUofql43Ccg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6502f910-fcc2-4b32-e960-0481526a6b4a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Hello\" log prob: -2.0794415416798357\n",
            "\"Hi my name is HAL\" log prob: -9.704060527839234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the Unigram model to classify text (but, may not have the highest accuracy)"
      ],
      "metadata": {
        "id": "-TrZT9jHUoJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(\n",
        "    file_id='1hyziAb67N3RuhhBSCtBhpQEPgr6ZIpJP',\n",
        "    dest_path='/tmp/emotion.csv'\n",
        ")\n",
        "df_full = pd.read_csv('/tmp/emotion.csv')\n",
        "df_train = df_full[df_full['subset']=='train'].copy()\n",
        "df_test = df_full[df_full['subset']=='test'].copy()\n",
        "label_key = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
        "\n",
        "# Init models\n",
        "total_count = len(df_train)\n",
        "label_counts = df_train['label'].value_counts().sort_index()\n",
        "models = [{\n",
        "     'index': i,\n",
        "     'label': label,\n",
        "     'log_prior': np.log(label_counts.iloc[i]/total_count),\n",
        "     'unigram_model': Unigram(),\n",
        "} for i, label in enumerate(label_key)]\n",
        "\n",
        "# Train models\n",
        "for model in models:\n",
        "  df_train_matching_label = df_train[df_train['label']==model['index']]\n",
        "  tokenized_sentences = df_train_matching_label['text'].apply(tokenize).tolist()\n",
        "  model['unigram_model'].train(tokenized_sentences)"
      ],
      "metadata": {
        "id": "DPmS3fwZBUce"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict classes\n",
        "def predict(sentence:str)->int:\n",
        "  tokenized_sentence = tokenize(sentence)\n",
        "  highest_log_prob = float('-inf')\n",
        "  highest_log_prob_index = 0\n",
        "  for model in models:\n",
        "    # !!!!!!!!!!!!! Your code starts here !!!!!!!!!!!!! \n",
        "    # Compute log prob of the sentence using the ungram model + the log prior of the label\n",
        "    log_prob = model['unigram_model'].sentence_log_prob(tokenized_sentence) + model['log_prior']\n",
        "    # !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!! \n",
        "    if log_prob > highest_log_prob:\n",
        "      highest_log_prob = log_prob\n",
        "      highest_log_prob_index = model['index']\n",
        "  return highest_log_prob_index\n",
        "\n",
        "df_test['predicted_label'] = df_test['text'].apply(predict)\n",
        "\n",
        "tp_count = sum(df_test['predicted_label']==df_test['label'])\n",
        "accuracy = tp_count/len(df_test)\n",
        "print(f'Accuracy: {accuracy*100}%')"
      ],
      "metadata": {
        "id": "vZaPT3ncVimN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dba64c5-aaca-421d-aced-a506acd01230"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 62.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2: Types of Language Models\n",
        "This sections explains different types of language models. We will go over 3 of the most used language model types:\n",
        "1. Causal\n",
        "2. Masked\n",
        "3. Sequence to sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ5Yovuqyyl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1: Causal language model\n",
        "\n",
        "A causal language model provides the probability of a token given the tokens before it\n",
        "\n",
        "$P(token_T|token_1,token_2,...,token_{T-1})$\n",
        "\n",
        "It is useful for a variety of NLP tasking including sequence generation and sequence classification\n",
        "\n",
        "Example:\n",
        "Hello, my name is ...\n",
        "\n",
        "Output:\n",
        "Hello, my name is HAL"
      ],
      "metadata": {
        "id": "f2tsj7E9gYRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, set_seed\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
      ],
      "metadata": {
        "id": "1lumqe-5hmr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10609de6-d628-4bae-b4f1-c6c9b86291b4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example"
      ],
      "metadata": {
        "id": "OyktiYVCmZpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gpt_text_greedy(input_text,sequence_max_length = 25):\n",
        "  \"\"\"\n",
        "  This uses greedy decoding which is not optimal for most tasks,\n",
        "  but requires little system resources and is simple to implement\n",
        "  \n",
        "  Recommended for deterministic tasks: Beam search\n",
        "  Recommended for creative tasks: Nucleus sampling\n",
        "  Recommended for low latency/real time tasks: Greedy decoding or Nucleus sampling\n",
        "\n",
        "  Note: Optimal decoding/generation algorithm depends on the task\n",
        "  \"\"\"\n",
        "  generated_text = input_text\n",
        "\n",
        "  # Generate a sequence\n",
        "  for i in tqdm(range(sequence_max_length)):\n",
        "    with torch.no_grad(): # Better preformance\n",
        "      inputs = tokenizer(generated_text, return_tensors=\"pt\")\n",
        "      outputs = model(**inputs)\n",
        "      next_token_logits = outputs.logits[0, -1, :]\n",
        "      next_token_index = torch.argmax(next_token_logits)\n",
        "      generated_text = tokenizer.decode(\n",
        "          torch.cat((inputs['input_ids'][0],torch.tensor([next_token_index]))) # generated_text = generated_text + new_token\n",
        "      )\n",
        "  \n",
        "  return generated_text\n",
        "\n",
        "input_text = \"Hello, my name is John Smith. I am the\"\n",
        "\n",
        "print(f'Generated: {generate_gpt_text_greedy(input_text)}')"
      ],
      "metadata": {
        "id": "d8elQsK3iPlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b89d847-03a6-4ed4-ebc7-81f6b7948bea"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:04<00:00,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: hello, my name is john smith. i am the head of the department of defense. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alt_text = \"John said it again three times: \\\"No No No\"\n",
        "print(f'Generated: {generate_gpt_text_greedy(alt_text)}')"
      ],
      "metadata": {
        "id": "sS2jVWsOwyWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1744ba-4368-4f1b-fdd0-4cb185a874c5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:05<00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: john said it again three times : \" no no no no no no no no no no no no no no no no no no no no no no no no no no no no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** b) \n",
        "\n",
        "i) What do you notice about the generated text?\n",
        "\n",
        "\n",
        "\n",
        "ii) How can this be avoided?\n"
      ],
      "metadata": {
        "id": "aZD2Ea56ozk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** c) (*CMPUT 566 Students Only*)\n",
        "Implement the Nucleas Sampling method described in Section 3.1 of https://arxiv.org/pdf/1904.09751.pdf. You can use any `torch` or `torch.nn` (`nn`) functions\n",
        "\n",
        "Hint: Use `torch.multinomial` for sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "BbcAaBa8KRwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=0)\n",
        "\n",
        "def generate_gpt_text_nucleus_sampling(input_text, sequence_max_length = 25, p=0.9):\n",
        "  \"\"\"\n",
        "  This uses greedy decoding which is not optimal for most tasks,\n",
        "  but requires little system resources and is simple to implement\n",
        "  \n",
        "  Recommended for deterministic tasks: Beam search\n",
        "  Recommended for creative tasks: Nucleus sampling\n",
        "  Recommended for low latency/real time tasks: Greedy decoding or Nucleus sampling\n",
        "\n",
        "  Note: Optimal decoding/generation algorithm depends on the task\n",
        "  \"\"\"\n",
        "  generated_text = input_text\n",
        "\n",
        "  # Generate a sequence\n",
        "  for i in tqdm(range(sequence_max_length)):\n",
        "    with torch.no_grad(): # Better preformance\n",
        "      inputs = tokenizer(generated_text, return_tensors=\"pt\")\n",
        "      outputs = model(**inputs)\n",
        "      # !!!!!!!!!!!!! Your code starts here !!!!!!!!!!!!! \n",
        "\n",
        "      # !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!! \n",
        "      generated_text = tokenizer.decode(\n",
        "          torch.cat((inputs['input_ids'][0],torch.tensor([next_token_index]))) # generated_text = generated_text + new_token\n",
        "      )\n",
        "  \n",
        "  return generated_text\n",
        "\n",
        "\n",
        "torch.manual_seed(314159)\n",
        "input_text = \"Hello, my name is John Smith. I am the\"\n",
        "\n",
        "print(f'Generated: {generate_gpt_text_nucleus_sampling(input_text)}')"
      ],
      "metadata": {
        "id": "Wr-Y_enP-Tjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "bdfe045d-6dc3-435f-9741-83bbc9c1a229"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/25 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d1eb6820163b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, my name is John Smith. I am the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Generated: {generate_gpt_text_nucleus_sampling(input_text)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-d1eb6820163b>\u001b[0m in \u001b[0;36mgenerate_gpt_text_nucleus_sampling\u001b[0;34m(input_text, sequence_max_length, p)\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;31m# !!!!!!!!!!!!! Your code ends here !!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       generated_text = tokenizer.decode(\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generated_text = generated_text + new_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'next_token_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Hello, my name is John Smith. I am the\", return_tensors=\"pt\")\n",
        "torch.cat((inputs['input_ids'][0],torch.tensor([1000])))"
      ],
      "metadata": {
        "id": "O0TOgPOdM18G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['input_ids']"
      ],
      "metadata": {
        "id": "UEqkl297NLWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2: Masked language models\n",
        "A masked language model provides the probability of a token given the tokens before it and after it (fill in the blanks)\n",
        "\n",
        "$P(token_T|token_1,...,token_{T-1},token_{T+1},...,token_{N})$\n",
        "\n",
        "It is useful for a variety of NLP tasking including sequence classification and grammar correction\n",
        "\n",
        "Example: Hello, my name is ...\n",
        "\n",
        "Output: Hello, my name is HAL"
      ],
      "metadata": {
        "id": "MPXE9k84pDPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "BWbeuI_lpId6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa77da1-9a19-41b0-840b-26ac35121038"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"The capital of Alberta is [MASK].\"\n",
        "\n",
        "def predict_mask(input_text):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    logits = model(**inputs).logits\n",
        "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "  return tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print('Input: ',example_text)\n",
        "print('Mask prediction: ',predict_mask(example_text))"
      ],
      "metadata": {
        "id": "TEZX8ibCpKkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4180761a-258c-4904-b683-4d7db513fe85"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  The capital of Alberta is [MASK].\n",
            "Mask prediction:  edmonton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** d) Use the `predict_mask` function and the `[MASK]` token to exract a fact from the language model(similar to the example above). Include your input and the model's prediction in your pdf report"
      ],
      "metadata": {
        "id": "rnxbATf3rhQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"The winner of World Cup 2022 is [MASK].\"\n",
        "\n",
        "print('Input: ',your_prompt)\n",
        "print('Mask prediction: ',predict_mask(your_prompt))"
      ],
      "metadata": {
        "id": "CYdADAH_zKtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77dfb2ed-1ea1-4d81-e133-195d3b473a8b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  The winner of World Cup 2022 is [MASK].\n",
            "Mask prediction:  azerbaijan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3: Sequence to sequence models\n",
        "A sequence to sequence models provides the probability of a token given the tokens before it and all tokens in another related sequence\n",
        "\n",
        "It is useful for a variety of NLP tasking including translation and summarization (primarily used for text generation)\n",
        "\n",
        "Example: Bonjour, je m'appelle HAL (French)\n",
        "\n",
        "Output: Hello, my name is HAL (English)"
      ],
      "metadata": {
        "id": "F1gABSXQr6hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
      ],
      "metadata": {
        "id": "M7cgFkVwzTXy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def t5_summarize(text, max_length=20):\n",
        "  # inference\n",
        "  input_ids = tokenizer(\n",
        "      f\"summarize: {text}\", return_tensors=\"pt\"\n",
        "  ).input_ids\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_length=max_length,\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# The Road Not Taken\n",
        "# Poem by Robert Frost (1916)\n",
        "# (Public Domain)\n",
        "poem = \"\"\"Two roads diverged in a yellow wood,\n",
        "And sorry I could not travel both\n",
        "And be one traveler, long I stood\n",
        "And looked down one as far as I could\n",
        "To where it bent in the undergrowth;\n",
        "\n",
        "Then took the other, as just as fair,\n",
        "And having perhaps the better claim,\n",
        "Because it was grassy and wanted wear;\n",
        "Though as for that the passing there\n",
        "Had worn them really about the same,\n",
        "\n",
        "And both that morning equally lay\n",
        "In leaves no step had trodden black.\n",
        "Oh, I kept the first for another day!\n",
        "Yet knowing how way leads on to way,\n",
        "I doubted if I should ever come back.\n",
        "\n",
        "I shall be telling this with a sigh\n",
        "Somewhere ages and ages hence:\n",
        "Two roads diverged in a wood, and I—\n",
        "I took the one less traveled by,\n",
        "And that has made all the difference.\"\"\"\n",
        "\n",
        "print('Summary: ',t5_summarize(poem))"
      ],
      "metadata": {
        "id": "DDohl2s1t1CH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d43b01-b5c0-40bc-9156-c3a16c1b2a43"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  two roads diverged in a yellow wood, and I took the one less traveled by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metaphors are lost on the model, but it still does a fairly good job summarizing the literal meaning of the poem"
      ],
      "metadata": {
        "id": "FoZUvx_T3Ibw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** e)\n",
        "\n",
        "i) Find a short piece of text (article, poem, section of a paper) and get the model to summarize it. Include the summary in your report"
      ],
      "metadata": {
        "id": "tlwv2vxo3RUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "short_text = \"\"\"\n",
        "Despite considerable advances in neural language modeling, it remains an open\n",
        "question what the best decoding strategy is for text generation from a language\n",
        "model (e.g. to generate a story). The counter-intuitive empirical observation is\n",
        "that even though the use of likelihood as training objective leads to high quality\n",
        "models for a broad range of language understanding tasks, maximization-based\n",
        "decoding methods such as beam search lead to degeneration — output text that is\n",
        "bland, incoherent, or gets stuck in repetitive loops.\n",
        "\"\"\"\n",
        "\n",
        "print('Summary: ',t5_summarize(short_text,max_length=35)) # You can change `max_length` if summary seems truncated"
      ],
      "metadata": {
        "id": "Lgt5aKf80JpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73dee15-c56e-4bd5-b0e9-4d4886f46afa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  decoding is a strategy that can be used to generate a story. the decoding method is a good example of a language model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Is the summary accurate? If yes, explain why the summary is accurate? If not, explain how the summary could be improved"
      ],
      "metadata": {
        "id": "r3TRScA81IWt"
      }
    }
  ]
}
